---
title: "AP4"
subtitle: "Data 607"
author: "T. Jenkins, J. Jeffryes, N. Chung"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

<center><img src="http://.jpg"></center>

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)

# general visualisation
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('gridExtra') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation

# general data manipulation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation

# specific visualisation
library('alluvial') # visualisation
library('ggrepel') # visualisation
library('ggridges') # visualisation
library('gganimate') # visualisation
library('ggExtra') # visualisation

# specific data manipulation
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('purrr') # string manipulation
library('reshape2') # data wrangling

# Text / NLP
library('tidytext') # text analysis
library('tm') # text analysis
library('SnowballC') # text analysis
library('topicmodels') # text analysis
library('wordcloud') # test visualisation
library('igraph') # visualisation
library('ggraph') # visualisation
library('babynames') # names

# Models
library('Matrix')
library('xgboost')
library('caret')

library('treemapify') #visualisation
library(rvest)
library(purrr)
library(lubridate)
library(readr)
library(tidytext)
library(widyr) # pairwise counting
library(gridExtra) #viewing multiple plots together
library(wordcloud2) #creative visualizations
library('readtext')
```


## Corpus Creation
#https://stackoverflow.com/questions/47410866/r-inspect-document-term-matrix-results-in-error-repeated-indices-currently-not

# this was not used except in the following cell but study to understand the data structures
#http://rpubs.com/Fixed_Point/486842
also see http://rstudio-pubs-static.s3.amazonaws.com/546059_4e95e2e13528417b968061fbb5d0e6ee.html
for coding style

See for a tfid plot
https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling

final note https://github.com/yanhann10/opendata_viz/tree/master/refugee

# Assignment
It can be useful to be able to classify new "test" documents using already classified "training" documents. A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). One example corpus:   https://spamassassin.apache.org/old/publiccorpus/

# Approach


# VCorpus
```{r}
# load data into objec ts
ham_rawc <- VCorpus(DirSource("data/easy_ham/"))
spam_rawc <- VCorpus(DirSource("data/spam/"))
 
## set the meta class
ham_corpus <- ham_rawc
spam_corpus <- spam_rawc
meta(ham_corpus, tag="class", type ="corpus") <- "ham"
meta(spam_corpus, tag="class", type ="corpus") <- "spam"

rm(list = c("ham_rawc", "spam_rawc")) # remove objects
gc() # garbage collection
 
# reference: format and view corpus
# https://www.rdocumentation.org/packages/tm/versions/0.7-6/topics/content_transformer
 
# collapse rows
collapse_lines <- content_transformer(function(x) paste(x, collapse="\n"))
ham_corpus <- tm_map(ham_corpus, collapse_lines)
spam_corpus <- tm_map(spam_corpus, collapse_lines)

# validate object contents
ham_corpus[[1]]$content[1]
```

## Set the docvars and remove header
```{r}
set_doc_vars <- function(x) {
  for(i in seq(1, length(x))){
    doc_content <-x[[i]]$content
    #print(doc_content)
    x[[i]]$meta["date"] <- str_extract(doc_content, "(?<=Date:)([^\\n]+)")
    ##print(x[[i]]$meta["date"])
    x[[i]]$meta["to"] <- str_extract(doc_content, "(?<=To:)([^\\n]+)")
    x[[i]]$meta["from"] <- str_extract(doc_content, "(?<=From:)([^\\n]+)")
    x[[i]]$meta["subject"] <- str_extract(doc_content, "(?<=Subject:)([^\\n]+)")
    
  }
  # Check Results
  # x[[2]]$meta
  return(x)
}

ham_corpus <- set_doc_vars(ham_corpus)
spam_corpus <- set_doc_vars(spam_corpus)
```


 
```{r}

#### Remove header
#Save for later stpes
# https://stackoverflow.com/questions/41109773/gsub-function-in-tm-package-to-remove-urls-does-not-remove-the-entire-string
#to_space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x)) 
c_ham <- tm_map(ham_corpus, content_transformer(function(x)  sub(".*?\n\n", "", x)))
c_spam <- tm_map(spam_corpus, content_transformer(function(x)  sub(".*?\n\n", "", x)))
```
 

# Check view the VCorpus and make Convenience features
```{r echo=FALSE}
view_hc_results <- function() {
  ham_c1 <- head(c_ham, n=1)
  hmeta_1 <- ham_c1[[1]][2]$meta
  print(ham_c1$meta$class) 
  print(names(ham_c1))
  hcontent_1 <- ham_c1[[1]][1]$content
  inspect(ham_c1)
  print(hmeta_1)
  cat(hcontent_1)
}
 

## Check and View the VCorpus
 
#How to get the content
#head(ham_corpus, n=2)$content
## Check corpus
#summary(test, showmeta=TRUE)

view_hc_results()

```



## PUT IN FUNCTION

### Data
#https://www.datacamp.com/community/tutorials/R-nlp-machine-learning
```{r}
fix_contractions <- function(doc) {
  # "won't" is a special case as it does not expand to "wo not"  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  # 's could be 'is' or could be possessive: it has no expansion  doc <- gsub("'s", "", doc)
  return(doc)
}

```



Good binning

```{r}
#create the decade column
#prince <- prince %>%
#mutate(decade =            ifelse(prince$year %in% 1978:1979, "1970s",            ifelse(prince$year %in% 1980:1989, "1980s",            ifelse(prince$year %in% 1990:1999, "1990s",            ifelse(prince$year %in% 2000:2009, "2000s",            ifelse(prince$year %in% 2010:2015, "2010s",                   "NA"))))))
```



### Clean Corpus


# Quanteda
to free memory
```{r}
library(quanteda)
library(RColorBrewer)

q_corp_ham <- tm::VCorpus(tm::VectorSource(ham_corpus))
q_corp_ham <- corpus(q_corp_ham)
q_corp_spam <- tm::VCorpus(tm::VectorSource(spam_corpus))
q_corp_spam <- corpus(q_corp_spam)
```


```{r}

#rm(list = c("ham_corpus", "spam_corpus"))
#gc()
```


#### Medium
#### Fix Contractions
```{r}
# fix (expand) contractions
c_ham <- tm_map(c_ham, content_transformer(fix_contractions))
c_spam <- tm_map(c_ham, content_transformer(fix_contractions))



c_ham <- tm_map(c_ham, content_transformer(tolower))
c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("https?://[^\\s]+", "", x)), lazy = TRUE)
##c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("\\\\", "", x)), lazy = TRUE)
c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("<.*?>", "", x)), lazy = TRUE)
c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("[[:punct:]]+"," ", x)), lazy = TRUE)
c_ham <- tm_map(c_ham, function(x) removePunctuation(x,
                                  preserve_intra_word_contractions = FALSE,
                                  preserve_intra_word_dashes = FALSE), lazy = TRUE)
#c_ham <- tm_map(c_ham, stripWhitespace, lazy = TRUE)





c_spam <- tm_map(c_spam, content_transformer(tolower))
c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("https?://[^\\s]+", "", x)))
##c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("\\\\", "", x)))
c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("<.[^>]+>", "", x)))
c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("[[:punct:]]+"," ", x)))
c_spam <- tm_map(c_spam, function(x) removePunctuation(x,
                                  preserve_intra_word_contractions = FALSE,
                                  preserve_intra_word_dashes = FALSE), lazy = TRUE)
#c_spam <- tm_map(c_spam, stripWhitespace, lazy = TRUE)
```

#### A lot
```{r}
c_ham <- tm_map(c_ham, removeNumbers, lazy = TRUE)
c_ham <- tm_map(c_ham, removeWords, stopwords("english"), lazy = TRUE)
#c_ham <- tm_map(c_ham, stemDocument, lazy = TRUE)
```

```{r}
c_spam <- tm_map(c_spam, removeNumbers, lazy = TRUE)
c_spam <- tm_map(c_spam, removeWords, stopwords("english"), lazy = TRUE)
#c_spam <- tm_map(c_spam, stemDocument, lazy = TRUE)


```

```{r echo=FALSE}
#Temp remove encasing function
#c_spam = clean_corpus(spam_corpus)
#c_ham = clean_corpus(ham_corpus)
```


### Look
```{r}
inspect(head(c_ham, n=1))
c_ham$meta
c_ham$meta$class
c_ham[[1]][1]$content
```

```{r}
# TODO: parse html
inspect(head(c_spam, n=1))
c_spam$meta
c_spam$meta$class
c_spam[[1]][1]$content
```

# Alternate methods
In this section, we load the spam and ham data into a common dataframe with columns "text", "class", and "id" for analysis and visualization.
```{r}
make_df <- function() {
  dfh <- data.frame(text = sapply(c(c_ham), as.character), stringsAsFactors = FALSE)
  dfh$class <- "ham"
  dfh$id <- rownames(dfh)
  glimpse(dfh) # validate ham output
  dfs <- data.frame(text = sapply(c(c_spam), as.character), stringsAsFactors = FALSE)
  dfs$class <- "spam"
  dfs$id <- rownames(dfs)
  glimpse(dfs) # validate spam output
  df <- rbind(dfh, dfs)
  glimpse(df) # validate merge
  df <- df %>%
    mutate(class = as.factor(class))
  return(df)
}
df <- make_df()
head(df) 
```

We utilize the `tidytext` scissors to drop punctuation, `stopwords`, and transform all words into lower case. 
```{r}
t1 <- df %>% unnest_tokens(word, text)

t1 <- t1 %>%
  anti_join(stop_words, by = "word")

head(t1$word)
```

We next filter the dataframe for each "ham" and "spam" to visualize the most common words in the text in a wordcloud.
```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 2", out.width="100%"}
t1 %>%
  filter(class == "ham") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 30, color = "purple4"))

t1 %>%
  filter(class == "spam") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 30, color = "red2"))
```

Here we demonstrate how to transform a dataframe into a Document Term Matrix
```{r}
t1_h <- t1 %>%
  filter(class == "ham")

t1_s <- t1 %>%
  filter(class == "spam")

t2_h <- Corpus(VectorSource(t1_h$word))
dtm_h <- DocumentTermMatrix(t2_h)

t2_s <- Corpus(VectorSource(t1_s$word))
dtm_s <- DocumentTermMatrix(t2_s)
print(dtm_h)
print(dtm_s)
```

We then visualize word frequency by classification, as well as word and sentence length.
```{r}
t1_group <- t1 %>%
  group_by(word, class) %>%
  count()

t1_word <- t1 %>%
  group_by(word) %>%
  count() %>%
  rename(all = n)

# plot word frequency by group
t1_group %>%
  left_join(t1_word, by = "word") %>%
  arrange(desc(all)) %>%
  head(80) %>%
  ungroup() %>%
  ggplot(aes(reorder(word, all, FUN = min), n, fill = class)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ class) +
  theme(legend.position = "none")

#detect sentences
p2 <- df %>%
  mutate(sen_len = str_length(text)) %>%
  ggplot(aes(sen_len, class, fill = class)) +
  geom_density_ridges() +
  scale_x_log10() +
  theme(legend.position = "none") +
  labs(x = "Sentence length [# characters]")

p3 <- t1 %>%
  mutate(word_len = str_length(word)) %>%
  ggplot(aes(word_len, fill = class)) +
  geom_density(bw = 0.05, alpha = 0.3) +
  scale_x_log10() +
  theme(legend.position = "none") +
  labs(x = "Word length [# characters]")

print(p2)
print(p3)
```

## Fix

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", out.width="100%"}
frequency <- t1 %>%
  count(class, word) %>%
  filter(n > 50) %>%
  group_by(class) %>%
  mutate(freq = n / sum(n)) %>%
  select(-n) %>%
  spread(class, freq) %>%
  gather(class, freq, spam:ham) %>%
  filter(!is.na(freq) & !is.na(class)) %>%
  arrange(desc(freq))

head(frequency)
```


```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", out.width="100%"}
ggplot(frequency, aes(freq, spam, color = abs(spam - freq))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.1, height = 0.1) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  #scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray95") +
  facet_wrap(~spam, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "spam", x = NULL)

```



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 6", out.width="100%"}

frequency <- t1 %>%

  count(class, word) %>%

  filter(n > 1e1) %>%

  group_by(class) %>%

  mutate(freq = n / sum(n)) %>%

  select(-n) %>%

  spread(class, freq)



frequency %>%

  select(-word) %>%

  cor(use="complete.obs", method="spearman") %>%

  corrplot(type="lower", method="number", diag=FALSE)

```
### Clean more Omit


#TFID
```{r}

frequency <-t1 %>%

  count(class, word)



tf_idf <- frequency %>%

  bind_tf_idf(word, class, n)

```

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 7", out.width="100%"}

tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(word = factor(word, levels = rev(unique(word)))) %>%

  top_n(30, tf_idf) %>%

  ggplot(aes(word, tf_idf, fill = class)) +

  geom_col() +

  labs(x = NULL, y = "TF-IDF values") +

  theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))



```




```{r}

t2 <- df %>% select(class, text) %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

sample_n(t2, 5)

```





```{r}

bi_sep <- t2 %>%

  separate(bigram, c("word1", "word2"), sep = " ")



bi_filt <- bi_sep %>%

  filter(!word1 %in% stop_words$word) %>%

  filter(!word2 %in% stop_words$word)



# for later

bigram_counts <- bi_filt %>%

  count(word1, word2, sort = TRUE)



t2 <- bi_filt %>%

  unite(bigram, word1, word2, sep = " ")

```


```{r}

t2_tf_idf <- t2 %>%

  count(class, bigram) %>%

  bind_tf_idf(bigram, class, n) %>%

  arrange(desc(tf_idf))

```




```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 9", out.width="100%"}

t2_tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%

  group_by(class) %>%

  top_n(10, tf_idf) %>%

  ungroup() %>%

  ggplot(aes(bigram, tf_idf, fill = class)) +

  geom_col() +

  labs(x = NULL, y = "TF-IDF values") +

  theme(legend.position = "none") +

  facet_wrap(~ class, ncol = 3, scales = "free") +

  coord_flip()

```




```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 10", out.width="100%"}

bigram_graph <- bigram_counts %>%

  filter(n > 6) %>%

  graph_from_data_frame()



set.seed(1234)



a <- grid::arrow(type = "closed", length = unit(.1, "inches"))



ggraph(bigram_graph, layout = "fr") +

  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,

                 arrow = a, end_cap = circle(.07, 'inches')) +

  geom_node_point(color = "lightblue", size = 3) +

  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +

  theme_void()

```





```{r}

# input parameters: class, minimum count for bigram graph

plot_bigram_net<- function(aname, bimin){



  foo <- t2 %>%

    filter(class == aname)



  bar <- foo %>%

    separate(bigram, c("word1", "word2"), sep = " ")



  bi_filt <- bar %>%

    filter(!word1 %in% stop_words$word) %>%

    filter(!word2 %in% stop_words$word)



  bigram_graph <- bi_filt %>%

    count(word1, word2, sort = TRUE) %>%

    filter(n > bimin) %>%

    graph_from_data_frame()



  set.seed(1234)



  a <- grid::arrow(type = "closed", length = unit(.1, "inches"))



  ggraph(bigram_graph, layout = "fr") +

    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,

                 arrow = a, end_cap = circle(.07, 'inches')) +

    geom_node_point(color = "lightblue", size = 3) +

    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +

    theme_void()

}



```

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 11", out.width="100%"}

plot_bigram_net("spam",4)
plot_bigram_net("ham",4)

```





```{r}

t3 <- df %>% select(class, text) %>% unnest_tokens(trigram, text, token = "ngrams", n = 3)



tri_sep <- t3 %>%

  separate(trigram, c("word1", "word2", "word3"), sep = " ")



tri_filt <- tri_sep %>%

  filter(!word1 %in% stop_words$word) %>%

  filter(!word2 %in% stop_words$word) %>%

  filter(!word3 %in% stop_words$word)



# for later

trigram_counts <- tri_filt %>%

  count(word1, word2, word3, sort = TRUE)



t3 <- tri_filt %>%

  unite(trigram, word1, word2, word3, sep = " ")



sample_n(t3, 5)

```





```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}

t3_tf_idf <- t3 %>%

  count(class, trigram) %>%

  bind_tf_idf(trigram, class, n) %>%

  arrange(desc(tf_idf))



t3_tf_idf %>%

  arrange(desc(tf_idf)) %>%

  mutate(trigram = factor(trigram, levels = rev(unique(trigram)))) %>%

  group_by(class) %>%

  top_n(5, tf_idf) %>%

  ungroup() %>%

  ggplot(aes(trigram, tf_idf, fill = class)) +

  geom_col() +

  labs(x = NULL, y = "TF-IDF values") +

  theme(legend.position = "none") +

  facet_wrap(~ class, ncol = 3, scales = "free") +

  coord_flip()

```





```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}

t1 %>%

  filter(class == "spam") %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  ungroup() %>%

  group_by(sentiment) %>%

  top_n(10, n) %>%

  ungroup() %>%

  mutate(word = reorder(word, n)) %>%

  ggplot(aes(word, n, fill = sentiment)) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~sentiment, scales = "free_y") +

  labs(y = "Contribution to negative/positive sentiment", x = NULL) +

  coord_flip() +

  ggtitle("Spam - Sentiment analysis")

```



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 15", out.width="100%"}

t1 %>%

  filter(class == "ham") %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  ungroup() %>%

  group_by(sentiment) %>%

  top_n(10, n) %>%

  ungroup() %>%

  mutate(word = reorder(word, n)) %>%

  ggplot(aes(word, n, fill = sentiment)) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~sentiment, scales = "free_y") +

  labs(y = "Contribution to negative/positive sentiment", x = NULL) +

  coord_flip() +

  ggtitle("Ham - Sentiment analysis")

```



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 17", out.width="100%"}

t1 %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  count(word, sentiment, sort = TRUE) %>%

  acast(word ~ sentiment, value.var = "n", fill = 0) %>%

  comparison.cloud(colors = c("#F8766D", "#00BFC4"), max.words = 50)

```




```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 18", fig.height=3.5, out.width="100%"}

p1 <- t1 %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  ggplot(aes(class, fill = sentiment)) +

  geom_bar(position = "fill")



p2 <- t1 %>%

  inner_join(get_sentiments("bing"), by = "word") %>%

  group_by(class, id, sentiment) %>%

  count() %>%

  spread(sentiment, n, fill = 0) %>%

  group_by(class, id) %>%

  summarise(neg = sum(negative),

            pos = sum(positive)) %>%

  arrange(id) %>%

  mutate(frac_neg = neg/(neg + pos)) %>%

  ggplot(aes(frac_neg, fill = class)) +

  geom_density(bw = .2, alpha = 0.3) +

  theme(legend.position = "right") +

  labs(x = "Fraction of negative words per sentence")


print(p1)
print(p2)

```




## Topics



```{r}

freq <-t1 %>%

  count(id, word)



t1_tm <- cast_dtm(freq, id, word, n)

t1_tm

```






```{r}

inspect(t1_tm[1:5,1:10])

```



##takes a while

```{r}

t1_lda <- LDA(t1_tm, k = 15, control = list(seed = 1234))

```






```{r}

t1_topics <- tidy(t1_lda, matrix = "beta")

t1_topics %>% sample_n(5)

```





```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 21", out.width="100%"}

t1_topics %>%

  group_by(topic) %>%

  top_n(5, beta) %>%

  ungroup() %>%

  arrange(topic, -beta) %>%

  mutate(term = reorder(term, beta)) %>%

  ggplot(aes(term, beta, fill = factor(topic))) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~ topic, scales = "free", ncol = 5) +

  coord_flip()

```


Let's visualise the top terms per topic:



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 21", out.width="100%"}

t1_topics %>%

  group_by(topic) %>%

  top_n(5, beta) %>%

  ungroup() %>%

  arrange(topic, -beta) %>%

  mutate(term = reorder(term, beta)) %>%

  ggplot(aes(term, beta, fill = factor(topic))) +

  geom_col(show.legend = FALSE) +

  facet_wrap(~ topic, scales = "free", ncol = 5) +

  coord_flip()

```

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 22", out.width="100%"}

p1 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic1 > .001 | topic2 > .001) %>%

  mutate(log_ratio = log10(topic2 / topic1)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 2 / topic 1") +

  coord_flip()



p2 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic2 > .001 | topic3 > .001) %>%

  mutate(log_ratio = log10(topic3 / topic2)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 3 / topic 2") +

  coord_flip()



p3 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic4 > .001 | topic3 > .001) %>%

  mutate(log_ratio = log10(topic4 / topic3)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 4 / topic 3") +

  coord_flip()



p4 <- t1_topics %>%

  mutate(topic = paste0("topic", topic)) %>%

  spread(topic, beta) %>%

  filter(topic4 > .001 | topic5 > .001) %>%

  mutate(log_ratio = log10(topic5 / topic4)) %>%

  group_by(direction = log_ratio > 0) %>%

  top_n(5, abs(log_ratio)) %>%

  ungroup() %>%

  mutate(term = reorder(term, log_ratio)) %>%

  ggplot(aes(term, log_ratio, fill = log_ratio > 0)) +

  geom_col() +

  theme(legend.position = "none") +

  labs(y = "Log ratio of beta in topic 5 / topic 4") +

  coord_flip()
plot(p1)
plot(p2)
plot(p3)

plot(p4)

p1<-1;p2<-1;p3<-1;p4<-1;
```



```{r}
BREAKR
#raw.data <- raw.data[sample(nrow(raw.data)),]
```


See https://cran.r-project.org/web/packages/corpus/vignettes/corpus.html for text tricks

See https://www.kaggle.com/ambarish/a-very-extensive-data-analysis-of-yelp for word taxomony networks
# Back to cleaning, can skip section


```{r}
#combined = tm:::c.VCorpus(c_spam,c_ham)
```


# Pick up analysis
## From R docs
### SPAM


```{r echo=FALSE}
make_df()
head(df)
```

```{r errors=FALSE, warnings=FALSE}
#Creating the Bag of Words model
#https://www.rdocumentation.org/packages/tm/versions/0.7-6/topics/Corpus


```
# TDF

```{r errors=FALSE, warnings=FALSE}
corpus <- VCorpus( VectorSource( df))
#wordCloud
library(wordcloud)
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.999)


dataset <- as.matrix(dtm)
word_freq <- sort(colSums(dataset),decreasing=TRUE)
dataset$class <- corpus$meta$class
dataset$class <- factor(dataset$class)#, levels = c(0, 1))
wfn <- names(word_freq)
d <- data.frame(word=wfn,freq=word_freq)
head(d)
```


```{r}
# https://rpubs.com/komalbachhuka/TM_visuals
library(ggthemes)
tdm <- TermDocumentMatrix( corpus, control = list( weighting = weightTf))

#tdm_bigram <-TermDocumentMatrix(text, control=list(tokenize=function(x) NGramTokenizer(x,Weka_control(min=2, max=2))

dataset <- as.matrix( tdm)
term.freq <- rowSums(dataset)
freq.df <- data.frame( word = names( term.freq), frequency = term.freq)
freq.df <- freq.df[ order( freq.df[, 2], decreasing = T),]
freq.df
# plot the terms by frequency
freq.df$ word <- factor( freq.df $ word,
                          levels = unique( as.character( freq.df $ word)))
ggplot( freq.df[ 1: 20,], aes( x = word, y = frequency)) +
  geom_bar( stat ="identity", fill ='darkred') +
  coord_flip() + theme_gdocs() +
  geom_text( aes( label = frequency), colour ="white", hjust = 1.25, size = 5.0)
```


```{r}
#head(df)
#corpus <- combine(c_spam, c_ham)
#head(corpus)
```





```{r}
library( plotrix)
head(dataset)
common.words <- subset( dataset, dataset[, 1] > 0 & dataset[, 2] > 0)
#tail( common.words)
#calculate the differences between the two columns of common words

difference <- abs( common.words[, 1] - common.words[, 2])

#combine the differences with the common words
common.words <- cbind( common.words, difference)
#sort by the difference column in decreasing order
common.words <- common.words[ order( common.words[, 3], decreasing = TRUE), ]
common.words
#select the top 25 words and create a data frame
top25.df <- data.frame( x = common.words[ 1: 25, 1],
                         y = common.words[ 1: 25, 2],
                         labels = rownames(common.words[ 1: 25, ]))
```

#py plot
```{r}
top25.df <- freq.df[ 1: 20,]
top25.df
pyramid.plot(top25.df$word, top25.df$frequency,
#change gap to show longer words
                          gap = 20,
              top.labels = c("Spam", "Words", "delta"),
              main = "Words in Common",
              laxlab = NULL, raxlab = NULL, unit = NULL)
```


# Wordclouds


```{r}

suppressWarnings(wordcloud(d$word, colors=c(3,4),random.color=FALSE, d$freq, min.freq=80))
```




```{r warnings=FALSE}
wordcloud2(d)
```


```{r warnings=FALSE}
letterCloud(df, word = "SPAM", size = 4)
```


```{r warnings=FALSE}
wordcloud2(df, figPath = "pig.png", size = 1.5,color = "pink")

#fig <- system.file("nija.jpg",package = "wordcloud2")

wordcloud2(df,  figPath = "nija.jpg",  size = 1.5,color = "white")

```




```{r}
inspect(stemDocument(c_spam[[1]]))
```


```{r}
summary(q_corp_ham, 5)
summary(q_corp_spam, 5)
```



```{r}
df_save <- df
```

text metrics https://www.kaggle.com/headsortails/watch-your-language-update-feature-engineering


# Train
```{r}
head(df_save)
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df_save))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df_save)), size = smp_size)

train <- df_save[train_ind, ]
test <- df_save[-train_ind, ]
```

```{r}
summary(train)
summary(test)
```
```{r}
print(c(max(train$sentence_id), max(test$sentence_id)))
```






# vignette
https://www.kaggle.com/vennaa/notebook-spam-text-message-classification-with-r

```{r}
names(raw.data) <- c("Category", "Message")
raw.data[1,]
set.seed(1912)
raw.data <- raw.data[sample(nrow(raw.data)),]

sms.corpus <- corpus(raw.data$Message)
docvars(sms.corpus) <- raw.data$Category
spam.plot <- corpus_subset(sms.corpus, docvar1 == "spam")
spam.plot <- dfm(spam.plot, tolower = TRUE, remove_punct = TRUE,  remove_numbers = TRUE, remove=stopwords("SMART"))
spam.col <- brewer.pal(10, "BrBG")
spam.cloud <- textplot_wordcloud(spam.plot, min.freq = 16, color = spam.col)
title("Spam Wordcloud", col.main = "grey14")
```

```{r}
raw.data <- data.frame(Message=df_save$content,category=df_save$class)
names(raw.data) <- c("Category", "Message")
raw.data[1,]

glimpse(df[1,])

set.seed(1912)
raw.data <- raw.data[sample(nrow(raw.data)),]
```


```{r}
df<- dfm(corpus(df_save), tolower = TRUE)
df <- dfm_trim(df, min_termfreqError = 5, min_docfreq = 3)
df <- dfm_weight(df)#, type = "tfidf")



df_train<-train
df_test<-test
classifier <- textmodel_NB(df_train, df_train$class)


predictions <- predict(classifier, newdata = df_test)
table(predictions$nb.predicted, df_test$class)
```



# Plots

```{r}
df_save <- df
summary(df)

colnames(df) <- c("content", "class_", "id")
glimpse(df[1,])

```


# PLots
See for nice plots https://www.datacamp.com/community/tutorials/R-nlp-machine-learning

```{r}
#randomize, not working
#df1 <- sample(df,nrow(df), replace = TRUE)
#glimpse(data.frame(df1))

# function to remove special characters
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
# remove special characters
df$text <- sapply(df$text, removeSpecialChars)

# convert everything to lower case
df$text <- sapply(df$text, tolower)

str(df, nchar.max = 1000)
write.csv(df, file = "spam_ham.csv")

undesirable_words <- c("http", "localhost", "spamasassin")
head(sample(stop_words$word, 15), 15)
```


```{r}
#unnest and remove stop, undesirable and short words
df_words_filtered <- df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3)

class(df_words_filtered)

dim(df_words_filtered)

library(kableExtra)

library(formattable) # for the color_tile function
 df_words_filtered %>%
#  filter(word == "race") %>%
  select(word) %>%
  arrange(word) %>%
#replace with freq
  top_n(10,word) %>%
#  mutate(class = color_tile("lightblue","lightblue")(class)) %>%
  mutate(word = color_tile("lightgreen","lightgreen")(word)) %>%
  kableExtra::kable("html", escape = FALSE, align = "c", caption = "Tokenized Format Example") %>%
  kable_styling(bootstrap_options =c("striped", "condensed", "bordered"),full_width = FALSE)


```

```{r}
full_word_count <- df %>%
  unnest_tokens(word, content) %>%
  group_by(class_,id) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))

full_word_count[1:10,] %>%
  ungroup(num_words, class_) %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  mutate(id = color_tile("lightpink","lightpink")(id)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Highest Word Count") %>%
  kable_styling(bootstrap_options =c("striped", "condensed", "bordered"),full_width = FALSE)

full_word_count %>%
  ggplot() +
    geom_histogram(aes(x = num_words))+#, fill = class )) +
    ylab(" Count") +     xlab("Word Count per ") +
    ggtitle("Word Count Distribution") +
    theme(plot.title = element_text(hjust = 0.5),
          legend.title = element_blank(),
          panel.grid.minor.y = element_blank())

```

```{r}
 popular_tfidf_words <- df %>%
  unnest_tokens(word, content) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(content, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, content, n)

head(popular_tfidf_words)
```

```{r}
top_popular_tfidf_words <- popular_tfidf_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(class) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(class, tf_idf) %>%
  mutate(row = row_number())


top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf,              fill = chart_level)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") +     ggtitle("Important Words using TF-IDF by Chart Level") +
    theme_lyrics() +      facet_wrap(~class, ncol = 3, scales = "free") +
    scale_x_continuous(  # This handles replacement of row
      breaks = top_popular_tfidf_words$row, # notice need to reuse data frame
      labels = top_popular_tfidf_words$word) +
    coord_flip()
```
```{r}
wc <- tfidf_words %>%
  arrange(desc(tf_idf)) %>%
  select(word, tf_idf)

wordcloud2(wc)
```
https://www.kaggle.com/vennaa/notebook-spam-text-message-classification-with-r
```{r}

```



# Notes
```{r}
read_html("") %>%
  html_nodes("a[href^='https://docs.goo']") %>%
  html_attr("href") %>%
  map_df(read_csv) %>%
  mutate(date=mdy(date)) -> sh
```

```{r}
pg <- read_html("")

html_nodes(pg, "a.btn.btn-default")
html_nodes(pg, "a[href^='https://docs.goo']")
html_nodes(pg, xpath=".//a[@class='btn btn-default']")
html_nodes(pg, xpath=".//a[contains(@href, 'https://docs.goo')]")
```

## Notes from elsehwhere

```{r warnings=FALSE}
# Transform the text to a tidy data structure with one token per row
tokens <- c_spam[[1]]$content %>%
  unnest_tokens(word, content)

# Positive and negative words
tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort=TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
```


```{r warnings=FALSE}
# Most frequent bigrams
ep4.bigrams <- frequentBigrams(c_spam$content)[1:20,]
ggplot(data=ep4.bigrams, aes(x=reorder(word, -freq), y=freq)) +
  geom_bar(stat="identity", fill="chocolate2", colour="black") +
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(x="Bigram", y="Frequency")


```

tidy
https://juliasilge.github.io/tidytext/articles/tidying_casting.html

more functions https://cran.r-project.org/web/packages/preText/vignettes/getting_started_with_preText.html
data https://spamassassin.apache.org/old/publiccorpus/

simple http://www.rpubs.com/AGoldberg/IS607W11

cleanNLP https://raw.githubusercontent.com/yanhann10/opendata_viz/master/refugee/refugee.Rmd
