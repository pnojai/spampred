---
title: "AP4"
subtitle: "Data 607"
author: "T. Jenkins, J. Jeffryes, N. Chung"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)

# general visualisation
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('gridExtra') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation

# general data manipulation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation

# specific visualisation
library('alluvial') # visualisation
library('ggrepel') # visualisation
library('ggridges') # visualisation
library('gganimate') # visualisation
library('ggExtra') # visualisation

# specific data manipulation
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('purrr') # string manipulation
library('reshape2') # data wrangling

# Text / NLP
library('tidytext') # text analysis
library('tm') # text analysis
library('SnowballC') # text analysis
library('topicmodels') # text analysis
library('wordcloud') # test visualisation
library('igraph') # visualisation
library('ggraph') # visualisation
library('babynames') # names

# Models
library('Matrix')
library('xgboost')
library('caret')

library('treemapify') #visualisation
library(rvest)
library(purrr)
library(lubridate)
library(readr)
library(tidytext)
library(widyr) # pairwise counting
library(gridExtra) #viewing multiple plots together
library(wordcloud2) #creative visualizations
library('readtext')
library(quanteda)
library(RColorBrewer)
```

# Assignment
It can be useful to be able to classify new "test" documents using already classified "training" documents. A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). One example corpus:   https://spamassassin.apache.org/old/publiccorpus/

# Approach
```{r}
# load data into objec ts
ham_rawc <- VCorpus(DirSource("data/easy_ham/"))
spam_rawc <- VCorpus(DirSource("data/spam/"))
 
## set the meta class
ham_corpus <- ham_rawc
spam_corpus <- spam_rawc
meta(ham_corpus, tag="class", type ="corpus") <- "ham"
meta(spam_corpus, tag="class", type ="corpus") <- "spam"

rm(list = c("ham_rawc", "spam_rawc")) # remove objects
gc() # garbage collection
 
# reference: format and view corpus
# https://www.rdocumentation.org/packages/tm/versions/0.7-6/topics/content_transformer
 
# collapse rows
collapse_lines <- content_transformer(function(x) paste(x, collapse="\n"))
ham_corpus <- tm_map(ham_corpus, collapse_lines)
spam_corpus <- tm_map(spam_corpus, collapse_lines)

# validate object contents
ham_corpus[[1]]$content[1]
```

Set the docvars and remove headers from the text
```{r}
set_doc_vars <- function(x) {
  for(i in seq(1, length(x))){
    doc_content <-x[[i]]$content
    #print(doc_content)
    x[[i]]$meta["date"] <- str_extract(doc_content, "(?<=Date:)([^\\n]+)")
    ##print(x[[i]]$meta["date"])
    x[[i]]$meta["to"] <- str_extract(doc_content, "(?<=To:)([^\\n]+)")
    x[[i]]$meta["from"] <- str_extract(doc_content, "(?<=From:)([^\\n]+)")
    x[[i]]$meta["subject"] <- str_extract(doc_content, "(?<=Subject:)([^\\n]+)")
    
  }
  # Check Results
  # x[[2]]$meta
  return(x)
}

ham_corpus <- set_doc_vars(ham_corpus)
spam_corpus <- set_doc_vars(spam_corpus)
```
 
```{r}

#### Remove header
#Save for later stpes
# https://stackoverflow.com/questions/41109773/gsub-function-in-tm-package-to-remove-urls-does-not-remove-the-entire-string
#to_space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x)) 
c_ham <- tm_map(ham_corpus, content_transformer(function(x)  sub(".*?\n\n", "", x)))
c_spam <- tm_map(spam_corpus, content_transformer(function(x)  sub(".*?\n\n", "", x)))
```
 

Check view the VCorpus and make Convenience features
```{r echo=FALSE}
view_hc_results <- function() {
  ham_c1 <- head(c_ham, n=1)
  hmeta_1 <- ham_c1[[1]][2]$meta
  print(ham_c1$meta$class) 
  print(names(ham_c1))
  hcontent_1 <- ham_c1[[1]][1]$content
  inspect(ham_c1)
  print(hmeta_1)
  cat(hcontent_1)
}
 

## Check and View the VCorpus
 
#How to get the content
#head(ham_corpus, n=2)$content
## Check corpus
#summary(test, showmeta=TRUE)

view_hc_results()

```

```{r}
fix_contractions <- function(doc) {
  # "won't" is a special case as it does not expand to "wo not"  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  # 's could be 'is' or could be possessive: it has no expansion  doc <- gsub("'s", "", doc)
  return(doc)
}

```

# Clean corpus
We use the "tm" and "quanteda" library to create collections of documents and corpora, respectively.
```{r}
q_corp_ham <- tm::VCorpus(tm::VectorSource(ham_corpus))
q_corp_ham <- corpus(q_corp_ham)
q_corp_spam <- tm::VCorpus(tm::VectorSource(spam_corpus))
q_corp_spam <- corpus(q_corp_spam)
```

```{r}
# fix (expand) contractions
c_ham <- tm_map(c_ham, content_transformer(fix_contractions))
c_spam <- tm_map(c_ham, content_transformer(fix_contractions))



c_ham <- tm_map(c_ham, content_transformer(tolower))
c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("https?://[^\\s]+", "", x)), lazy = TRUE)
##c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("\\\\", "", x)), lazy = TRUE)
c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("<.*?>", "", x)), lazy = TRUE)
c_ham <- tm_map(c_ham, content_transformer(function(x) gsub("[[:punct:]]+"," ", x)), lazy = TRUE)
c_ham <- tm_map(c_ham, function(x) removePunctuation(x,
                                  preserve_intra_word_contractions = FALSE,
                                  preserve_intra_word_dashes = FALSE), lazy = TRUE)
#c_ham <- tm_map(c_ham, stripWhitespace, lazy = TRUE)





c_spam <- tm_map(c_spam, content_transformer(tolower))
c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("https?://[^\\s]+", "", x)))
##c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("\\\\", "", x)))
c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("<.[^>]+>", "", x)))
c_spam <- tm_map(c_spam, content_transformer(function(x) gsub("[[:punct:]]+"," ", x)))
c_spam <- tm_map(c_spam, function(x) removePunctuation(x,
                                  preserve_intra_word_contractions = FALSE,
                                  preserve_intra_word_dashes = FALSE), lazy = TRUE)
#c_spam <- tm_map(c_spam, stripWhitespace, lazy = TRUE)
```

```{r}
c_ham <- tm_map(c_ham, removeNumbers, lazy = TRUE)
c_ham <- tm_map(c_ham, removeWords, stopwords("english"), lazy = TRUE)
#c_ham <- tm_map(c_ham, stemDocument, lazy = TRUE)
```

```{r}
c_spam <- tm_map(c_spam, removeNumbers, lazy = TRUE)
c_spam <- tm_map(c_spam, removeWords, stopwords("english"), lazy = TRUE)
#c_spam <- tm_map(c_spam, stemDocument, lazy = TRUE)


```

```{r echo=FALSE}
#Temp remove encasing function
#c_spam = clean_corpus(spam_corpus)
#c_ham = clean_corpus(ham_corpus)
```

```{r}
inspect(head(c_ham, n=1))
c_ham$meta
c_ham$meta$class
c_ham[[1]][1]$content
```

```{r}
# TODO: parse html
inspect(head(c_spam, n=1))
c_spam$meta
c_spam$meta$class
c_spam[[1]][1]$content
```

# Analysis
In this section, we load the spam and ham data into a common dataframe with columns "text", "class", and "id" for analysis and visualization.
```{r}
make_df <- function() {
  dfh <- data.frame(text = sapply(c(c_ham), as.character), stringsAsFactors = FALSE)
  dfh$class <- "ham"
  dfh$id <- rownames(dfh)
  glimpse(dfh) # validate ham output
  dfs <- data.frame(text = sapply(c(c_spam), as.character), stringsAsFactors = FALSE)
  dfs$class <- "spam"
  dfs$id <- rownames(dfs)
  glimpse(dfs) # validate spam output
  df <- rbind(dfh, dfs)
  glimpse(df) # validate merge
  df <- df %>%
    mutate(class = as.factor(class))
  return(df)
}
df <- make_df()
head(df) 
```

We utilize the `tidytext` scissors to drop punctuation, `stopwords`, and transform all words into lower case. 
```{r}
t1 <- df %>% unnest_tokens(word, text)

t1 <- t1 %>%
  anti_join(stop_words, by = "word")

head(t1$word)
```

We next filter the dataframe for each "ham" and "spam" to visualize the most common words in the text in a wordcloud.
```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 2", out.width="100%"}
t1 %>%
  filter(class == "ham") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 30, color = "purple4"))

t1 %>%
  filter(class == "spam") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 30, color = "red2"))
```

Here we demonstrate how to transform a dataframe into a Document Term Matrix
```{r}
t1_h <- t1 %>%
  filter(class == "ham")

t1_s <- t1 %>%
  filter(class == "spam")

t2_h <- Corpus(VectorSource(t1_h$word))
dtm_h <- DocumentTermMatrix(t2_h)

t2_s <- Corpus(VectorSource(t1_s$word))
dtm_s <- DocumentTermMatrix(t2_s)
print(dtm_h)
print(dtm_s)
```

We then visualize word frequency by classification, as well as word and sentence length.
```{r}
t1_group <- t1 %>%
  group_by(word, class) %>%
  count()

t1_word <- t1 %>%
  group_by(word) %>%
  count() %>%
  rename(all = n)

# plot word frequency by group
t1_group %>%
  left_join(t1_word, by = "word") %>%
  arrange(desc(all)) %>%
  head(80) %>%
  ungroup() %>%
  ggplot(aes(reorder(word, all, FUN = min), n, fill = class)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ class) +
  theme(legend.position = "none")

#detect sentences
p2 <- df %>%
  mutate(sen_len = str_length(text)) %>%
  ggplot(aes(sen_len, class, fill = class)) +
  geom_density_ridges() +
  scale_x_log10() +
  theme(legend.position = "none") +
  labs(x = "Sentence length [# characters]")

p3 <- t1 %>%
  mutate(word_len = str_length(word)) %>%
  ggplot(aes(word_len, fill = class)) +
  geom_density(bw = 0.05, alpha = 0.3) +
  scale_x_log10() +
  theme(legend.position = "none") +
  labs(x = "Word length [# characters]")

print(p2)
print(p3)
```

## Fix

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", out.width="100%"}
frequency <- t1 %>%
  count(class, word) %>%
  filter(n > 50) %>%
  group_by(class) %>%
  mutate(freq = n / sum(n)) %>%
  select(-n) %>%
  spread(class, freq) %>%
  gather(class, freq, spam:ham) %>%
  filter(!is.na(freq) & !is.na(class)) %>%
  arrange(desc(freq))

head(frequency)
```


```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", out.width="100%"}
ggplot(frequency, aes(freq, spam, color = abs(spam - freq))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.1, height = 0.1) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  #scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray95") +
  facet_wrap(~spam, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "spam", x = NULL)

```

# TFID
```{r}

frequency <-t1 %>%

  count(class, word)



tf_idf <- frequency %>%

  bind_tf_idf(word, class, n)

```


```{r}
t2 <- df %>% select(class, text) %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
sample_n(t2, 5)
```

```{r}
bi_sep <- t2 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bi_filt <- bi_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# for later
bigram_counts <- bi_filt %>%
  count(word1, word2, sort = TRUE)

t2 <- bi_filt %>%
  unite(bigram, word1, word2, sep = " ")
```

```{r}
t2_tf_idf <- t2 %>%
  count(class, bigram) %>%
  bind_tf_idf(bigram, class, n) %>%
  arrange(desc(tf_idf))
```

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 9", out.width="100%"}
t2_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
  group_by(class) %>%
  top_n(10, tf_idf) %>%
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = class)) +
  geom_col() +
  labs(x = NULL, y = "TF-IDF values") +
  theme(legend.position = "none") +
  facet_wrap(~ class, ncol = 3, scales = "free") +
  coord_flip()
```

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 10", out.width="100%"}
bigram_graph <- bigram_counts %>%
  filter(n > 6) %>%
  graph_from_data_frame()

set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```


```{r}
# input parameters: class, minimum count for bigram graph
plot_bigram_net<- function(aname, bimin){
  foo <- t2 %>%
    filter(class == aname)

  bar <- foo %>%
    separate(bigram, c("word1", "word2"), sep = " ")

  bi_filt <- bar %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word)

  bigram_graph <- bi_filt %>%
    count(word1, word2, sort = TRUE) %>%
    filter(n > bimin) %>%
    graph_from_data_frame()

  set.seed(1234)

  a <- grid::arrow(type = "closed", length = unit(.1, "inches"))
  
  ggraph(bigram_graph, layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "lightblue", size = 3) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```

```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 11", out.width="100%"}

plot_bigram_net("spam",4)
plot_bigram_net("ham",4)

```



```{r}
t3 <- df %>% select(class, text) %>% unnest_tokens(trigram, text, token = "ngrams", n = 3)

tri_sep <- t3 %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

tri_filt <- tri_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# for later
trigram_counts <- tri_filt %>%
  count(word1, word2, word3, sort = TRUE)

t3 <- tri_filt %>%
  unite(trigram, word1, word2, word3, sep = " ")

sample_n(t3, 5)

```


```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}
t3_tf_idf <- t3 %>%
  count(class, trigram) %>%
  bind_tf_idf(trigram, class, n) %>%
  arrange(desc(tf_idf))

t3_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(trigram = factor(trigram, levels = rev(unique(trigram)))) %>%
  group_by(class) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  ggplot(aes(trigram, tf_idf, fill = class)) +
  geom_col() +
  labs(x = NULL, y = "TF-IDF values") +
  theme(legend.position = "none") +
  facet_wrap(~ class, ncol = 3, scales = "free") +
  coord_flip()
```

We perform sentiment anlaysis to analyze the content of each class of emails.
```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 14", out.width="100%"}
t1 %>%
  filter(class == "spam") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to negative/positive sentiment", x = NULL) +
  coord_flip() +
  ggtitle("Spam - Sentiment analysis")
```



```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 15", out.width="100%"}
t1 %>%
  filter(class == "ham") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to negative/positive sentiment", x = NULL) +
  coord_flip() +
  ggtitle("Ham - Sentiment analysis")
```


```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 17", out.width="100%"}
t1 %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"), max.words = 50)

```


```{r split=FALSE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 18", fig.height=3.5, out.width="100%"}
p1 <- t1 %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  ggplot(aes(class, fill = sentiment)) +
  geom_bar(position = "fill")

p2 <- t1 %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  group_by(class, id, sentiment) %>%
  count() %>%
  spread(sentiment, n, fill = 0) %>%
  group_by(class, id) %>%
  summarise(neg = sum(negative),
            pos = sum(positive)) %>%
  arrange(id) %>%
  mutate(frac_neg = neg/(neg + pos)) %>%
  ggplot(aes(frac_neg, fill = class)) +
  geom_density(bw = .2, alpha = 0.3) +
  theme(legend.position = "right") +
  labs(x = "Fraction of negative words per sentence")

print(p1)
print(p2)
```

# References
* https://juliasilge.github.io/tidytext/articles/tidying_casting.html
* https://cran.r-project.org/web/packages/preText/vignettes/getting_started_with_preText.html
* https://spamassassin.apache.org/old/publiccorpus/
* https://raw.githubusercontent.com/yanhann10/opendata_viz/master/refugee/refugee.Rmd
* https://www.datacamp.com/community/tutorials/R-nlp-machine-learning
