---
title: "Project 4"
author: "Jai Jeffryes (first draft)"
date: "11/3/2019"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(stringr)
library(dplyr)
library(tidyr)
library(tidytext)
# library(widyr) # pairwise counting
library(ggplot2)
library(tm)
```

## Corpus trial (not executed)
Tried a few steps of building a corpus using `tm`. Turned this off in favor of a Tidyverse workflow.

```{r corpus_exp, eval=FALSE}
dir_spam <- "./data/spam"
dir_ham <- "./data/easy_ham"

fils_ham <- list.files(dir_ham, full.names = TRUE)
fils_spam <- list.files(dir_spam, full.names = TRUE)

create_corpus <- function(fils, class) {
	for (i in seq_along(fils)) {
		raw_text <- read_file(fils[i])
		if (i == 1) {
			corpus <- VCorpus(VectorSource(raw_text))
		} else {
		corpus_1doc <- VCorpus(VectorSource(raw_text))
		corpus <- c(corpus, corpus_1doc)
		}
		# meta(ret_Corpus, tag = "class", type = "indexed") <- class
	}
	meta(corpus, tag = "class") <- class
	return(corpus)
}

corpus_ham <- create_corpus(fils_ham, "ham")
corpus_spam <- create_corpus(fils_spam, "spam")
```

## Read emails
The Tidy approach starts here.

*Reference*: [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com).

Read two directories of emails. `readr::read_file()` is very fast and syntactically easier than `base::readLines()`.

```{r}
# Directories for emails.
dir_spam <- "./data/spam"
dir_ham <- "./data/easy_ham"

# File lists.
fils_ham <- list.files(dir_ham, full.names = TRUE)
fils_spam <- list.files(dir_spam, full.names = TRUE)

# Function for input.
get_text_df <- function(files, class) {
	raw_email <- sapply(files, read_file)
	# Pick off key value on end of file names
	id <- str_extract(files, "\\w*$")
	# Corpus in tidy format. Each row is a document.
	text_df <- tibble(id = id, class = class, email = raw_email)

	return(text_df)
}

# Input.
ham_df <- get_text_df(fils_ham, "ham")
spam_df <- get_text_df(fils_spam, "spam")

# Merge the documents into one corpus.
email_df <- rbind(ham_df, spam_df)

# Free some memory, if you care.
rm(list = c("ham_df", "spam_df"))
gc()
```

## Clean
```{r}
# Remove headers.
# RegEx notes:
# (?s)(.*?\\n\\n)(.*)
# (?s) - Single line mode. Lets EOL match the dot.
# (.*?\\n\\n) - Non-greedy matching of any text and the first double line break.
# (.*) Body of the email.
email_bodies <- lapply(email_df$email, function(x) {
    str_match(x, "(?s)(.*?\\n\\n)(.*)")[3]})
# Convert list to data frame.
email_bodies <- do.call(rbind.data.frame, email_bodies)
colnames(email_bodies) <- "email"
email_bodies$email <- as.character(email_bodies$email)

# Replace email column with bodies.
email_df <- cbind(email_df[, 1:2], email_bodies)

# More cleaning.
# email_df$email <- removePunctuation(email_df$email)
# I don't want to remove punctuation. I think URLs differentiate emails.
# email_df$email <- removeNumbers(email_df$email)
# email_df$email <- stripWhitespace(email_df$email)
# email_df$email <- tolower(email_df$email)

```

## Tokenize
This approach creates a tall, narrow data frame of tokens.

Remove stop words.

```{r}
# Look at 2 emails.
# dplyr retrieval is easy. Filtering and aggregation, too.
email_df %>% 
	select(email) %>% 
	sample_n(2)

# Tokenize all emails.
email_tokens_df <- email_df %>% 
	unnest_tokens(word, email) %>% 
	anti_join(stop_words)
```

### Look at some counts.

**Possible additional cleansing**

- Stemming.

```{r}
email_tokens_df %>% 
    
	count(word, sort = TRUE)
```


## TD-IDF
Calculate term frequency and and inverse document frequency.

```{r}
email_tf_idf <- email_tokens_df %>% 
  count(class, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, class, n)

email_tf_idf %>% 
  arrange(-tf_idf)

email_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(class) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~class, ncol = 2, scales = "free") +
  coord_flip()
```

## Document term matrix
First departure from Tidy formats. This should be usable by `tm`.

```{r}
# Word counts.
word_counts <- email_tokens_df %>%
  # anti_join(stop_words) %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

# Cast to a document term matrix.
email_dtm <- word_counts %>% 
	cast_dtm(id, word, n)
```

These actions work on the Tidy generated DTM.

```{r}
email_dtm <- removeSparseTerms(email_dtm, 0.90)
freq_terms <- Terms(email_dtm)
ft <- colSums(as.matrix(email_dtm))
ft_df <- data.frame(term = names(ft), count = as.integer(ft))
knitr::kable(head(ft_df[order(ft, decreasing = TRUE), ], n = 20L),
             row.names = FALSE)
```


