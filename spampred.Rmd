---
title: "Project 4"
author: "Jai Jeffryes (first draft)"
date: "11/3/2019"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(stringr)
library(dplyr)
library(tibble)
library(tidyr)
library(tidytext)
# library(widyr) # pairwise counting
library(ggplot2)
library(tm)
library(caret)
```

## Read emails
The Tidy approach starts here.

*Reference*: [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com).

Read two directories of emails. `readr::read_file()` is very fast and syntactically easier than `base::readLines()`.

```{r}
# Directories for emails.
dir_spam <- "./data/spam"
dir_ham <- "./data/easy_ham"

# File lists.
fils_ham <- list.files(dir_ham, full.names = TRUE)
fils_spam <- list.files(dir_spam, full.names = TRUE)

# Filter files for development.
max_fils <- 100
fils_ham <- fils_ham[1:(min(max_fils, length(fils_ham)))]
fils_spam <- fils_spam[1:(min(max_fils, length(fils_spam)))]

# Function for input.
get_text_df <- function(files, class) {
	raw_email <- sapply(files, read_file)
	# Pick off key value on end of file names
	id <- str_extract(files, "\\w*$")
	# Corpus in tidy format. Each row is a document.
	text_df <- tibble(id = id, class = as.factor(class), email = raw_email)

	return(text_df)
}

# Input.
ham_df <- get_text_df(fils_ham, "ham")
spam_df <- get_text_df(fils_spam, "spam")

# Merge the documents into one corpus.
email_df <- rbind(ham_df, spam_df)

# Free some memory, if you care.
rm(list = c("ham_df", "spam_df"))
gc()
```

## Clean
Remove headers.

```{r}
# Remove headers.
# RegEx notes:
# (?s)(.*?\\n\\n)(.*)
# (?s) - Single line mode. Lets EOL match the dot.
# (.*?\\n\\n) - Non-greedy matching of any text and the first double line break.
# (.*) Body of the email.
# Tibbles. They're great. No strings as factors, no row names.
email_bodies <- tibble(email = sapply(email_df$email, function(x) {
     str_match(x, "(?s)(.*?\\n\\n)(.*)")[3]}))

# Replace email column with bodies.
email_df <- cbind(email_df[, 1:2], email_bodies)
```

## Tokenize
This approach creates a tall, narrow data frame of tokens.

We get some cleaning for free.

- Remove punctuation.
- Convert to lower case.
- Remove white space.

And we outer join a stop word list to remove those.

- Remove stop words.

```{r}
# Look at 2 emails.
# dplyr retrieval is easy. Filtering and aggregation, too.
email_df %>% 
	select(email) %>% 
	sample_n(2)

# Tokenize all emails.
email_tokens_df <- email_df %>% 
	unnest_tokens(word, email) %>% 
	anti_join(stop_words)
```

### Look at some counts.

**Possible additional cleansing**

- Stemming.

```{r}
email_tokens_df %>% 
    
	count(word, sort = TRUE)
```


## TD-IDF
Calculate term frequency and and inverse document frequency.

```{r}
email_tf_idf <- email_tokens_df %>% 
  count(class, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, class, n)

email_tf_idf %>% 
  arrange(-tf_idf)

email_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(class) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~class, ncol = 2, scales = "free") +
  coord_flip()
```

## Document term matrix
First departure from Tidy formats. Usable by ML models.

Cast counts to a DTM. Inspect and reduce some of the sparseness.

```{r}
# Word counts.
word_counts <- email_tokens_df %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

# Cast to a document term matrix.
email_dtm <- word_counts %>% 
	cast_dtm(id, word, n)

# Print
dim(email_dtm)
email_dtm
inspect(email_dtm[101:105, 101:105])

# inspect() does the work of printing the object and a matrix conversion.
email_dtm
email_m <- as.matrix(email_dtm)
dim(email_m)
email_m[101:105, 101:105]

# Remove some sparse terms and review again.
email_dtm_rm_sparse <- removeSparseTerms(email_dtm, 0.99)
dim(email_dtm_rm_sparse)
email_dtm_rm_sparse
inspect(email_dtm_rm_sparse[101:105, 101:105])

```

These actions work on the Tidy generated DTM.

```{r eval=FALSE}
#email_dtm <- removeSparseTerms(email_dtm, 0.90)
freq_terms <- Terms(email_dtm)
ft <- colSums(as.matrix(email_dtm))
ft_df <- data.frame(term = names(ft), count = as.integer(ft))
knitr::kable(head(ft_df[order(ft, decreasing = TRUE), ], n = 20L),
             row.names = FALSE)
```

## Assign train and test sets
```{r}
ham_cnt <- nrow(subset(email_df, class == "ham"))
spam_cnt <- nrow(subset(email_df, class == "spam"))

set.seed(2012)
train_indicator_ham <- rbinom(n = ham_cnt, size = 1, prob = .5)
train_indicator_spam <- rbinom(n = spam_cnt, size = 1, prob = .5)

# We discarded the split corpuses when we merged them. Split them again
# to assign a training indicator.
email_df_ham <- email_df %>% 
    filter(class == "ham") %>% 
    mutate(train_indicator = train_indicator_ham)

email_df_spam <- email_df %>% 
    filter(class == "spam") %>% 
    mutate(train_indicator = train_indicator_spam)

email_df <- rbind(email_df_ham, email_df_spam)

table(email_df$class, email_df$train_indicator)

email_dtm_df <- as.data.frame(as.matrix(email_dtm_rm_sparse))

email_dtm_df <- email_dtm_df %>% 
    rownames_to_column(var = "doc_id")

email_dtm_df <- inner_join(email_df, email_dtm_df, by = c("id" ="doc_id")) %>% 
    select(-c("id", "email.x"))

train_dtm <- subset(email_dtm_df, train_indicator == 1)
train_dtm <- train_dtm %>% select(-"train_indicator")
test_dtm <- subset(email_dtm_df, train_indicator == 0)
test_dtm <- test_dtm %>% select(-"train_indicator")
```

## Support Vector Machine
```{r}
train_model <- train(class.x ~ ., data = train_dtm, method = 'svmLinear3')
predict_train = predict(train_model, newdata=train_dtm)
predict_test = predict( train_model, newdata=test_dtm)
```

## Results
Observe confusion matrices.

```{r}
svm_confusion_m_train <- confusionMatrix(predict_train, train_dtm$class.x)
svm_confusion_m <- confusionMatrix(predict_test, test_dtm$class.x)

svm_confusion_m_train
svm_confusion_m
```

