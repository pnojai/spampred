---
title: "Project 4"
author: "Jai Jeffryes, Tamiko Jenkins, Nicholas Chung"
date: "11/3/2019"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Flag spam
Analyze a corpus of emails classified as spam or legitimate (dubbed "ham"). Develop a predictive process for classifying new email correctly.

```{r}
library(readr)
library(stringr)
library(dplyr)
library(tibble)
library(tidyr)
library(tidytext)
library(ggplot2)
library(tm)
library(caret)
```

## Approach
The central structure required for this text mining process is a Document Term Matrix (DTM), which will be the input into machine learning models. There are multiple alternatives for producing a DTM. They include:

- Building a corpus with the `tm` package.
- The `quanteda` package.
- Tidyverse tools.

Each of them work, and once you have the DTM in hand, you can use it with any ML model.

We elected the Tidyverse, which embraced data input, cleansing, and aggregation into a DTM.

*Reference*: [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com).

## Read emails
The Tidy approach starts here.

Read two directories of emails. `readr::read_file()` is very fast and syntactically easier than `base::readLines()`.

```{r}
# Directories for emails.
dir_spam <- "data/spam"
dir_ham <- "data/easy_ham"

# File lists.
fils_ham <- list.files(dir_ham, full.names = TRUE)
fils_spam <- list.files(dir_spam, full.names = TRUE)

# Filter files for development.
max_fils <- 500
fils_ham <- fils_ham[1:(min(max_fils, length(fils_ham)))]
fils_spam <- fils_spam[1:(min(max_fils, length(fils_spam)))]

# Function for input.
get_text_df <- function(files, class) {
	raw_email <- sapply(files, read_file)
	# Pick off key value on end of file names
	id <- str_extract(files, "\\w*$")
	# Corpus in tidy format. Each row is a document.
	text_df <- tibble(id = id, class = as.factor(class), email = raw_email)

	return(text_df)
}

# Input.
ham_df <- get_text_df(fils_ham, "ham")
spam_df <- get_text_df(fils_spam, "spam")

# Merge the documents into one corpus.
email_df <- rbind(ham_df, spam_df)

# Free some memory.
rm(list = c("ham_df", "spam_df"))
gc()
```

## Clean
Remove headers. By specification, email headers may not contain blank lines. The regular expression separates messages into capture groups.

RegEx notes:

- Full expression: (?s)(.*?\\n\\n)(.*)
- (?s) - Single line mode. Lets EOL match the dot.
- (.*?\\n\\n) - Non-greedy matching of any text and the first double line break.
- (.*) Body of the email.

```{r}
# Remove headers.
# Tibbles. They're great. No strings as factors, no row names.
email_bodies <- tibble(email = sapply(email_df$email, function(x) {
    # str_match(x, "(?s)(.*?\\n\\n)(.*)")[3]}))
    sub(".*?\n\n", "", x)
}))

# Replace email column with bodies.
email_df <- cbind(email_df[, 1:2], email_bodies)
```

## Tokenize
This approach creates a tall, narrow data frame of tokens.

We get some cleaning for free frum the `unnest()` function.

- Remove punctuation.
- Convert to lower case.
- Remove white space.

And we outer join a stop word list to remove those.

**Possible additional cleansing**

- Stemming.

```{r}
# Look at 2 emails.
# dplyr retrieval is easy. Filtering and aggregation, too.
email_df %>% 
	select(email) %>% 
	sample_n(2)

# Tokenize all emails.
email_tokens_df <- email_df %>% 
	unnest_tokens(word, email) %>% 
	anti_join(stop_words)
```

### Look at some counts.
```{r}
email_tokens_df %>% 
	count(word, sort = TRUE)
```

## TD-IDF
Calculate term frequency and and inverse document frequency. We'll need this for producing a DTM.

```{r}
email_tf_idf <- email_tokens_df %>% 
  count(class, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, class, n)

email_tf_idf %>% 
  arrange(-tf_idf)

email_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(class) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = class)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~class, ncol = 2, scales = "free") +
  coord_flip()
```

## Document term matrix
First departure from Tidy formats. Usable by ML models.

Cast counts to a DTM. Inspect and reduce some of the sparseness.

```{r}
# Word counts.
word_counts <- email_tokens_df %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

# Cast to a document term matrix.
email_dtm <- word_counts %>% 
	cast_dtm(id, word, n)

# Print
dim(email_dtm)
email_dtm
inspect(email_dtm[101:105, 101:105])

# inspect() does the work of printing the object and a matrix conversion.
email_dtm
email_m <- as.matrix(email_dtm)
dim(email_m)
email_m[101:105, 101:105]

# Remove some sparse terms and review again.
email_dtm_rm_sparse <- removeSparseTerms(email_dtm, 0.99)
dim(email_dtm_rm_sparse)
email_dtm_rm_sparse
inspect(email_dtm_rm_sparse[101:105, 101:105])

```

## Assign train and test sets
```{r}
ham_cnt <- nrow(subset(email_df, class == "ham"))
spam_cnt <- nrow(subset(email_df, class == "spam"))

set.seed(2012)
train_indicator_ham <- rbinom(n = ham_cnt, size = 1, prob = .5)
train_indicator_spam <- rbinom(n = spam_cnt, size = 1, prob = .5)

# We discarded the split corpuses when we merged them. Split them again
# to assign a training indicator.
email_df_ham <- email_df %>% 
    filter(class == "ham") %>% 
    mutate(train_indicator = train_indicator_ham)

email_df_spam <- email_df %>% 
    filter(class == "spam") %>% 
    mutate(train_indicator = train_indicator_spam)

email_df <- rbind(email_df_ham, email_df_spam)

table(email_df$class, email_df$train_indicator)

# We need to add the class into the matrix.
email_dtm_df <- as.data.frame(as.matrix(email_dtm_rm_sparse))

# The matrix conversion puts the id column into row names. The tibble package
# has a function for moving that into a column. There is a term for "id" in the matrix, so we need a synonym.
email_dtm_df <- email_dtm_df %>% 
    rownames_to_column(var = "doc_id")

# Join the dtm to the email data frame to pick up the class. Don't need
# the document id or the raw email, though.
email_dtm_df <- inner_join(email_df, email_dtm_df, by = c("id" ="doc_id")) %>% 
    select(-c("id", "email.x"))

# Build the train and test sets. Drop the indicators when you're done.
train_dtm <- subset(email_dtm_df, train_indicator == 1)
train_dtm <- train_dtm %>% select(-"train_indicator")
test_dtm <- subset(email_dtm_df, train_indicator == 0)
test_dtm <- test_dtm %>% select(-"train_indicator")
```

## Support Vector Machine
This email prediction is a supervised learning task. We employ SVM for its classification functionality. We train the model with a subset of the DTM, and test the prediction of ham/spam classification using the remainder.

```{r}
train_model <- train(class.x ~ ., data = train_dtm, method = 'svmLinear3')
predict_train = predict(train_model, newdata=train_dtm)
predict_test = predict( train_model, newdata=test_dtm)
```

## Results
We observe confusion matrices. First, we examine the confusion matrix for the training set just for its interest. It naturally gets a perfect score.

The confusion matrix for the test set is very accurate with a low p value.

```{r}
svm_confusion_m_train <- confusionMatrix(predict_train, train_dtm$class.x)
svm_confusion_m <- confusionMatrix(predict_test, test_dtm$class.x)

svm_confusion_m_train
svm_confusion_m
```

